var documenterSearchIndex = {"docs":
[{"location":"apis/#APIs","page":"APIs","title":"APIs","text":"","category":"section"},{"location":"apis/","page":"APIs","title":"APIs","text":"CurrentModule = ClimaComms","category":"page"},{"location":"apis/","page":"APIs","title":"APIs","text":"ClimaComms","category":"page"},{"location":"apis/#ClimaComms.ClimaComms","page":"APIs","title":"ClimaComms.ClimaComms","text":"ClimaComms\n\nAbstracts the communications interface for the various CliMA components in order to:\n\nsupport different computational backends (CPUs, GPUs)\nenable the use of different backends as transports (MPI, SharedArrays, etc.), and\ntransparently support single or double buffering for GPUs, depending on whether the transport has the ability to access GPU memory.\n\n\n\n\n\n","category":"module"},{"location":"apis/#Loading","page":"APIs","title":"Loading","text":"","category":"section"},{"location":"apis/","page":"APIs","title":"APIs","text":"ClimaComms.@import_required_backends\nClimaComms.cuda_is_required\nClimaComms.mpi_is_required","category":"page"},{"location":"apis/#ClimaComms.@import_required_backends","page":"APIs","title":"ClimaComms.@import_required_backends","text":"ClimaComms.@import_required_backends\n\nIf the desired context is MPI (as determined by ClimaComms.context()), try loading MPI.jl. If the desired device is CUDA (as determined by ClimaComms.device()), try loading CUDA.jl.\n\n\n\n\n\n","category":"macro"},{"location":"apis/#ClimaComms.cuda_is_required","page":"APIs","title":"ClimaComms.cuda_is_required","text":"cuda_is_required()\n\nReturns a Bool indicating if CUDA should be loaded, based on the ENV[\"CLIMACOMMS_DEVICE\"]. See ClimaComms.device for more information.\n\ncuda_is_required() && using CUDA\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.mpi_is_required","page":"APIs","title":"ClimaComms.mpi_is_required","text":"mpi_is_required()\n\nReturns a Bool indicating if MPI should be loaded, based on the ENV[\"CLIMACOMMS_CONTEXT\"]. See ClimaComms.context for more information.\n\nmpi_is_required() && using MPI\n\n\n\n\n\n","category":"function"},{"location":"apis/#Devices","page":"APIs","title":"Devices","text":"","category":"section"},{"location":"apis/","page":"APIs","title":"APIs","text":"ClimaComms.AbstractDevice\nClimaComms.AbstractCPUDevice\nClimaComms.CPUSingleThreaded\nClimaComms.CPUMultiThreaded\nClimaComms.CUDADevice\nClimaComms.device\nClimaComms.device_functional\nClimaComms.array_type\nClimaComms.allowscalar\nClimaComms.@time\nClimaComms.@elapsed\nClimaComms.@assert\nClimaComms.@sync\nClimaComms.@cuda_sync\nAdapt.adapt_structure(::Type{<:AbstractArray}, ::ClimaComms.AbstractDevice)\nClimaComms.@threaded\nClimaComms.@interdependent\nClimaComms.InterdependentIteratorData\nClimaComms.@sync_interdependent\nClimaComms.synchronize_gpu_threads\nClimaComms.static_shared_memory_array","category":"page"},{"location":"apis/#ClimaComms.AbstractDevice","page":"APIs","title":"ClimaComms.AbstractDevice","text":"AbstractDevice\n\nThe base type for a device.\n\n\n\n\n\n","category":"type"},{"location":"apis/#ClimaComms.AbstractCPUDevice","page":"APIs","title":"ClimaComms.AbstractCPUDevice","text":"AbstractCPUDevice()\n\nAbstract device type for single-threaded and multi-threaded CPU runs.\n\n\n\n\n\n","category":"type"},{"location":"apis/#ClimaComms.CPUSingleThreaded","page":"APIs","title":"ClimaComms.CPUSingleThreaded","text":"CPUSingleThreaded()\n\nUse the CPU with single thread.\n\n\n\n\n\n","category":"type"},{"location":"apis/#ClimaComms.CPUMultiThreaded","page":"APIs","title":"ClimaComms.CPUMultiThreaded","text":"CPUMultiThreaded()\n\nUse the CPU with multiple thread.\n\n\n\n\n\n","category":"type"},{"location":"apis/#ClimaComms.CUDADevice","page":"APIs","title":"ClimaComms.CUDADevice","text":"CUDADevice()\n\nUse NVIDIA GPU accelarator\n\n\n\n\n\n","category":"type"},{"location":"apis/#ClimaComms.device","page":"APIs","title":"ClimaComms.device","text":"ClimaComms.device()\n\nDetermine the device to use depending on the CLIMACOMMS_DEVICE environment variable.\n\nAllowed values:\n\nCPU, single-threaded or multi-threaded depending on the number of threads;\nCPUSingleThreaded,\nCPUMultiThreaded,\nCUDA.\n\nThe default is CPU.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.device_functional","page":"APIs","title":"ClimaComms.device_functional","text":"ClimaComms.device_functional(device)\n\nReturn true when the device is correctly set up.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.array_type","page":"APIs","title":"ClimaComms.array_type","text":"ClimaComms.array_type(::AbstractDevice)\n\nThe base array type used by the specified device (currently Array or CuArray).\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.allowscalar","page":"APIs","title":"ClimaComms.allowscalar","text":"allowscalar(f, ::AbstractDevice, args...; kwargs...)\n\nDevice-flexible version of CUDA.@allowscalar.\n\nLowers to\n\nf(args...)\n\nfor CPU devices and\n\nCUDA.@allowscalar f(args...)\n\nfor CUDA devices.\n\nThis is usefully written with closures via\n\nallowscalar(device) do\n    f()\nend\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.@time","page":"APIs","title":"ClimaComms.@time","text":"@time device expr\n\nDevice-flexible @time.\n\nLowers to\n\n@time expr\n\nfor CPU devices and\n\nCUDA.@time expr\n\nfor CUDA devices.\n\n\n\n\n\n","category":"macro"},{"location":"apis/#ClimaComms.@elapsed","page":"APIs","title":"ClimaComms.@elapsed","text":"@elapsed device expr\n\nDevice-flexible @elapsed.\n\nLowers to\n\n@elapsed expr\n\nfor CPU devices and\n\nCUDA.@elapsed expr\n\nfor CUDA devices.\n\n\n\n\n\n","category":"macro"},{"location":"apis/#ClimaComms.@assert","page":"APIs","title":"ClimaComms.@assert","text":"@assert device cond [text]\n\nDevice-flexible @assert.\n\nLowers to\n\n@assert cond [text]\n\nfor CPU devices and\n\nCUDA.@cuassert cond [text]\n\nfor CUDA devices.\n\n\n\n\n\n","category":"macro"},{"location":"apis/#ClimaComms.@sync","page":"APIs","title":"ClimaComms.@sync","text":"@sync device expr\n\nDevice-flexible @sync.\n\nLowers to\n\n@sync expr\n\nfor CPU devices and\n\nCUDA.@sync expr\n\nfor CUDA devices.\n\nAn example use-case of this might be:\n\nBenchmarkTools.@benchmark begin\n    if ClimaComms.device() isa ClimaComms.CUDADevice\n        CUDA.@sync begin\n            launch_cuda_kernels_or_spawn_tasks!(...)\n        end\n    elseif ClimaComms.device() isa ClimaComms.CPUMultiThreading\n        Base.@sync begin\n            launch_cuda_kernels_or_spawn_tasks!(...)\n        end\n    end\nend\n\nIf the CPU version of the above example does not leverage spawned tasks (which require using Base.sync or Threads.wait to synchronize), then you may want to simply use @cuda_sync.\n\n\n\n\n\n","category":"macro"},{"location":"apis/#ClimaComms.@cuda_sync","page":"APIs","title":"ClimaComms.@cuda_sync","text":"@cuda_sync device expr\n\nDevice-flexible CUDA.@sync.\n\nLowers to\n\nexpr\n\nfor CPU devices and\n\nCUDA.@sync expr\n\nfor CUDA devices.\n\n\n\n\n\n","category":"macro"},{"location":"apis/#Adapt.adapt_structure-Tuple{Type{<:AbstractArray}, ClimaComms.AbstractDevice}","page":"APIs","title":"Adapt.adapt_structure","text":"Adapt.adapt_structure(::Type{<:AbstractArray}, device::AbstractDevice)\n\nAdapt a given device to a device associated with the given array type.\n\nExample\n\nAdapt.adapt_structure(Array, ClimaComms.CUDADevice()) -> ClimaComms.CPUSingleThreaded()\n\nnote: Note\nBy default, adapting to Array creates a CPUSingleThreaded device, and there is currently no way to conver to a CPUMultiThreaded device.\n\n\n\n\n\n","category":"method"},{"location":"apis/#ClimaComms.@threaded","page":"APIs","title":"ClimaComms.@threaded","text":"@threaded [device] [coarsen=...] [block_size=...] for ... end\n\nDevice-flexible generalization of Threads.@threads, which distributes the iterations of a for-loop across multiple threads, with the option to control thread coarsening and GPU kernel configuration. Coarsening makes each thread evaluate more than one iteration of the loop, which can improve performance by reducing the runtime overhead of launching additional threads (though too much coarsening worsens performance because it reduces parallelization). The device is either inferred by calling ClimaComms.device(), or it can be specified manually, with the following device-dependent behavior:\n\nWhen device is a CPUSingleThreaded(), the loop is evaluated as-is. This avoids the runtime overhead of calling Threads.@threads with a single thread, and, when the device type is statically inferrable, it also avoids compilation overhead.\nWhen device is a CPUMultiThreaded(), the loop is passed to Threads.@threads. This supports three different kinds of \"schedulers\" for determining how many iterations of the loop to evaluate in each thread:\n(default) a \"dynamic\" scheduler that changes the number of iterations as new threads are launched,\na \"static\" scheduler that evaluates a fixed number of iterations per thread, and\na \"greedy\" scheduler that uses a small number of threads, continuously evaluating iterations in each thread until the loop is completed (only available as of Julia 1.11).\nSetting coarsen to :dynamic or :greedy launches threads with those schedulers. Setting it to :static or an integer value launches threads with static scheduling (using :static is similar to using 1, but slightly more performant). To read more about multi-threading, see the documentation for Threads.@threads.\nWhen device is a CUDADevice(), the loop is compiled with CUDA.@cuda and run with CUDA.@sync. Since CUDA launches all threads at the same time, only static scheduling can be used. Setting coarsen to any symbol causes each thread to evaluate a single iteration (default), and setting it to an integer value causes each thread to evaluate that number of iterations (the default is similar to using 1, but slightly more performant). If the total number of iterations in the loop is extremely large, the specified coarsening may require more threads than can be simultaneously launched on the GPU, in which case the amount of coarsening is automatically increased.\nThe optional argument block_size is also available for manually specifying the size of each block on a GPU. The default value of :auto sets the number of threads in each block to the largest possible value that permits a high GPU \"occupancy\" (the number of active thread warps in each multiprocessor executing the kernel). An integer can be used instead of :auto to override this default value. If the specified value exceeds the total number of threads, it is automatically decreased to avoid idle threads.\n\nNOTE: When a value in the body of the loop has a type that cannot be inferred by the compiler, an InvalidIRError will be thrown during compilation for a CUDADevice(). In particular, global variables are not inferrable, so @threaded must be wrapped in a function whenever it is used in the REPL:\n\njulia> a = CUDA.CuArray{Int}(undef, 100); b = similar(a);\n\njulia> threaded_copyto!(a, b) = ClimaComms.@threaded for i in axes(a, 1)\n           a[i] = b[i]\n       end\nthreaded_copyto! (generic function with 1 method)\n\njulia> threaded_copyto!(a, b)\n\njulia> ClimaComms.@threaded for i in axes(a, 1)\n           a[i] = b[i]\n       end\nERROR: InvalidIRError: ...\n\nMoreover, type variables are not inferrable across function boundaries, so types used in a threaded loop cannot be precomputed before the loop:\n\njulia> threaded_add_epsilon!(a) = ClimaComms.@threaded for i in axes(a, 1)\n           FT = eltype(a)\n           a[i] += eps(FT)\n       end\nthreaded_add_epsilon! (generic function with 1 method)\n\njulia> threaded_add_epsilon!(a)\n\njulia> function threaded_add_epsilon!(a)\n           FT = eltype(a)\n           ClimaComms.@threaded for i in axes(a, 1)\n               a[i] += eps(FT)\n           end\n       end\nthreaded_add_epsilon! (generic function with 1 method)\n\njulia> threaded_add_epsilon!(a)\nERROR: InvalidIRError: ...\n\nTo fix other kinds of inference issues on GPUs, especially ones brought about by indexing into iterators with nonuniform element types, see UnrolledUtilities.jl.\n\n\n\n\n\n","category":"macro"},{"location":"apis/#ClimaComms.@interdependent","page":"APIs","title":"ClimaComms.@interdependent","text":"@interdependent(itr)\n\nAnnotation for an iterator of an @threaded loop that allows elements of the iterator to affect each other within the loop. If an unannotated iterator is used in an @threaded loop, its elements are assumed to be independent (naively parallelizable). Items from an @interdependent iterator can only be used within @sync_interdependent expressions, ensuring that all interdependencies are isolated by thread synchronizations on GPUs. Since threads cannot be efficiently synchronized on CPUs, all items in an @interdependent iterator are processed on a single CPU thread, and each @sync_interdependent expression is transformed into a separate for-loop.\n\nThis annotation can be used in conjunction with static_shared_memory_array to implement performant kernels with interdependent threads in a device-agnostic way, e.g.,\n\njulia> first_axis_derivative!(device, a, ::Val{N}) where {N} =\n           ClimaComms.@threaded device for i in @interdependent(1:N), j in axes(a, 2)\n               a_j = ClimaComms.static_shared_memory_array(device, eltype(a), N)\n               ClimaComms.@sync_interdependent a_j[i] = a[i, j]\n               ClimaComms.@sync_interdependent if i == 1\n                   a[1, j] = a_j[2] - a_j[1]\n               elseif i == N\n                   a[N, j] = a_j[N] - a_j[N - 1]\n               else\n                   a[i, j] = (a_j[i + 1] - 2 * a_j[i] + a_j[i - 1]) / 2\n               end\n           end\nfirst_axis_derivative! (generic function with 1 method)\n\njulia> AT = ClimaComms.array_type(ClimaComms.device()); # Array or CuArray\n\njulia> a = AT(rand(100, 1000));\n\njulia> first_axis_derivative!(ClimaComms.device(), a, Val(100))\n\nA @threaded loop can have at most one independent iterator and one interdependent iterator. When two iterators are specified, as in the example above, the independent iterator is parallelized across GPU blocks, and the interdependent iterator is parallelized across threads within each block. Passing a single integer value to coarsen will only apply thread coarsening to the independent iterator when there are two iterators, but coarsen can also be specified as a pair of integers, in which case the second integer controls thread coarsening of the interdependent iterator.\n\n\n\n\n\n","category":"macro"},{"location":"apis/#ClimaComms.InterdependentIteratorData","page":"APIs","title":"ClimaComms.InterdependentIteratorData","text":"InterdependentIteratorData\n\nIntermediate representation of an item from an @interdependent iterator. Within each @sync_interdependent expression, this is replaced by a concrete value and can be used as if it were in a standard for-loop.\n\n\n\n\n\n","category":"type"},{"location":"apis/#ClimaComms.@sync_interdependent","page":"APIs","title":"ClimaComms.@sync_interdependent","text":"@sync_interdependent [item] code\n\nSynchronizes a segment of code within an @threaded loop, allowing the specified item from an @interdependent iterator to be used as if it were in a standard for-loop. If this macro is used called from the same lexical scope as its @threaded loop, the item variable can be determined automatically.\n\nOn GPU devices, the threads in every block are synchronized at the end of each @sync_interdependent expression. In coarsened GPU threads, the code is transformed into a for-loop over several items from the @interdependent iterator, after which the threads in every block are synchronized. On CPU devices, the code is transformed into a for-loop over the entire iterator.\n\n\n\n\n\n","category":"macro"},{"location":"apis/#ClimaComms.synchronize_gpu_threads","page":"APIs","title":"ClimaComms.synchronize_gpu_threads","text":"synchronize_gpu_threads(device)\n\nSynchronizes GPU threads with access to the same shared memory arrays. On CPU devices, synchronize_gpu_threads does nothing. Every @sync_interdependent expression is automatically followed by a call to this function on GPUs.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.static_shared_memory_array","page":"APIs","title":"ClimaComms.static_shared_memory_array","text":"static_shared_memory_array(device, T, dims...)\n\nDevice-flexible array whose element type T and dimensions dims are known during compilation, which corresponds to a static shared memory array on GPUs. On CPUs, which do not provide access to high-performance shared memory, this corresponds to an MArray instead. A static_shared_memory_array is much faster to access and modify than generic arrays like Array or CuArray. In @threaded loops, each static_shared_memory_array is shared by all threads with the same independent iterator items.\n\n\n\n\n\n","category":"function"},{"location":"apis/#Contexts","page":"APIs","title":"Contexts","text":"","category":"section"},{"location":"apis/","page":"APIs","title":"APIs","text":"ClimaComms.AbstractCommsContext\nClimaComms.SingletonCommsContext\nClimaComms.MPICommsContext\nClimaComms.AbstractGraphContext\nClimaComms.context\nClimaComms.graph_context\nAdapt.adapt_structure(::Type{<:AbstractArray}, ::ClimaComms.AbstractCommsContext)","category":"page"},{"location":"apis/#ClimaComms.AbstractCommsContext","page":"APIs","title":"ClimaComms.AbstractCommsContext","text":"AbstractCommsContext\n\nThe base type for a communications context. Each backend defines a concrete subtype of this.\n\n\n\n\n\n","category":"type"},{"location":"apis/#ClimaComms.SingletonCommsContext","page":"APIs","title":"ClimaComms.SingletonCommsContext","text":"SingletonCommsContext(device=device())\n\nA singleton communications context, used for single-process runs. ClimaComms.AbstractCPUDevice and ClimaComms.CUDADevice device options are currently supported.\n\n\n\n\n\n","category":"type"},{"location":"apis/#ClimaComms.MPICommsContext","page":"APIs","title":"ClimaComms.MPICommsContext","text":"MPICommsContext()\nMPICommsContext(device)\nMPICommsContext(device, comm)\n\nA MPI communications context, used for distributed runs. AbstractCPUDevice and CUDADevice device options are currently supported.\n\n\n\n\n\n","category":"type"},{"location":"apis/#ClimaComms.AbstractGraphContext","page":"APIs","title":"ClimaComms.AbstractGraphContext","text":"AbstractGraphContext\n\nA context for communicating between processes in a graph.\n\n\n\n\n\n","category":"type"},{"location":"apis/#ClimaComms.context","page":"APIs","title":"ClimaComms.context","text":"ClimaComms.context(device=device())\n\nConstruct a default communication context.\n\nBy default, it will try to determine if it is running inside an MPI environment variables are set; if so it will return a MPICommsContext; otherwise it will return a SingletonCommsContext.\n\nBehavior can be overridden by setting the CLIMACOMMS_CONTEXT environment variable to either MPI or SINGLETON.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.graph_context","page":"APIs","title":"ClimaComms.graph_context","text":"graph_context(context::AbstractCommsContext,\n              sendarray, sendlengths, sendpids,\n              recvarray, recvlengths, recvpids)\n\nConstruct a communication context for exchanging neighbor data via a graph.\n\nArguments:\n\ncontext: the communication context on which to construct the graph context.\nsendarray: array containing data to send\nsendlengths: list of lengths of data to send to each process ID\nsendpids: list of processor IDs to send\nrecvarray: array to receive data into\nrecvlengths: list of lengths of data to receive from each process ID\nrecvpids: list of processor IDs to receive from\n\nThis should return an AbstractGraphContext object.\n\n\n\n\n\n","category":"function"},{"location":"apis/#Adapt.adapt_structure-Tuple{Type{<:AbstractArray}, ClimaComms.AbstractCommsContext}","page":"APIs","title":"Adapt.adapt_structure","text":"Adapt.adapt_structure(::Type{<:AbstractArray}, context::AbstractCommsContext)\n\nAdapt a given context to a context with a device associated with the given array type.\n\nExample\n\nAdapt.adapt_structure(Array, ClimaComms.context(ClimaComms.CUDADevice())) -> ClimaComms.CPUSingleThreaded()\n\nnote: Note\nBy default, adapting to Array creates a CPUSingleThreaded device, and there is currently no way to conver to a CPUMultiThreaded device.\n\n\n\n\n\n","category":"method"},{"location":"apis/#Logging","page":"APIs","title":"Logging","text":"","category":"section"},{"location":"apis/","page":"APIs","title":"APIs","text":"ClimaComms.OnlyRootLogger\nClimaComms.MPILogger\nClimaComms.FileLogger","category":"page"},{"location":"apis/#ClimaComms.OnlyRootLogger","page":"APIs","title":"ClimaComms.OnlyRootLogger","text":"OnlyRootLogger()\nOnlyRootLogger(ctx::AbstractCommsContext)\n\nReturn a logger that silences non-root processes.\n\nIf no context is passed, obtain the default context via context.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.MPILogger","page":"APIs","title":"ClimaComms.MPILogger","text":"MPILogger(context::AbstractCommsContext)\nMPILogger(iostream, context)\n\nAdd a rank prefix before log messages.\n\nOutputs to stdout if no IOStream is given.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.FileLogger","page":"APIs","title":"ClimaComms.FileLogger","text":"FileLogger(context, log_dir; log_stdout = true, min_level = Logging.Info)\n\nLog MPI ranks to different files within the log_dir.\n\nThe minimum logging level is set using min_level. If log_stdout = true, root process logs will be sent to stdout as well.\n\n\n\n\n\n","category":"function"},{"location":"apis/#Context-operations","page":"APIs","title":"Context operations","text":"","category":"section"},{"location":"apis/","page":"APIs","title":"APIs","text":"ClimaComms.init\nClimaComms.mypid\nClimaComms.iamroot\nClimaComms.nprocs\nClimaComms.abort","category":"page"},{"location":"apis/#ClimaComms.init","page":"APIs","title":"ClimaComms.init","text":"(pid, nprocs) = init(ctx::AbstractCommsContext)\n\nPerform any necessary initialization for the specified backend. Return a tuple of the processor ID and the number of participating processors.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.mypid","page":"APIs","title":"ClimaComms.mypid","text":"mypid(ctx::AbstractCommsContext)\n\nReturn the processor ID.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.iamroot","page":"APIs","title":"ClimaComms.iamroot","text":"iamroot(ctx::AbstractCommsContext)\n\nReturn true if the calling processor is the root processor.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.nprocs","page":"APIs","title":"ClimaComms.nprocs","text":"nprocs(ctx::AbstractCommsContext)\n\nReturn the number of participating processors.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.abort","page":"APIs","title":"ClimaComms.abort","text":"abort(ctx::CC, status::Int) where {CC <: AbstractCommsContext}\n\nTerminate the caller and all participating processors with the specified status.\n\n\n\n\n\n","category":"function"},{"location":"apis/#Collective-operations","page":"APIs","title":"Collective operations","text":"","category":"section"},{"location":"apis/","page":"APIs","title":"APIs","text":"ClimaComms.barrier\nClimaComms.reduce\nClimaComms.reduce!\nClimaComms.allreduce\nClimaComms.allreduce!\nClimaComms.bcast","category":"page"},{"location":"apis/#ClimaComms.barrier","page":"APIs","title":"ClimaComms.barrier","text":"barrier(ctx::CC) where {CC <: AbstractCommsContext}\n\nPerform a global synchronization across all participating processors.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.reduce","page":"APIs","title":"ClimaComms.reduce","text":"reduce(ctx::CC, val, op) where {CC <: AbstractCommsContext}\n\nPerform a reduction across all participating processors, using op as the reduction operator and val as this rank's reduction value. Return the result to the first processor only.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.reduce!","page":"APIs","title":"ClimaComms.reduce!","text":"reduce!(ctx::CC, sendbuf, recvbuf, op)\nreduce!(ctx::CC, sendrecvbuf, op)\n\nPerforms elementwise reduction using the operator op on the buffer sendbuf, storing the result in the recvbuf of the process. If only one sendrecvbuf buffer is provided, then the operation is performed in-place.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.allreduce","page":"APIs","title":"ClimaComms.allreduce","text":"allreduce(ctx::CC, sendbuf, op)\n\nPerforms elementwise reduction using the operator op on the buffer sendbuf, allocating a new array for the result. sendbuf can also be a scalar, in which case recvbuf will be a value of the same type.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.allreduce!","page":"APIs","title":"ClimaComms.allreduce!","text":"allreduce!(ctx::CC, sendbuf, recvbuf, op)\nallreduce!(ctx::CC, sendrecvbuf, op)\n\nPerforms elementwise reduction using the operator op on the buffer sendbuf, storing the result in the recvbuf of all processes in the group. Allreduce! is equivalent to a Reduce! operation followed by a Bcast!, but can lead to better performance. If only one sendrecvbuf buffer is provided, then the operation is performed in-place.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.bcast","page":"APIs","title":"ClimaComms.bcast","text":"bcast(ctx::AbstractCommsContext, object)\n\nBroadcast object from the root process to all other processes. The value of object on non-root processes is ignored.\n\n\n\n\n\n","category":"function"},{"location":"apis/#Graph-exchange","page":"APIs","title":"Graph exchange","text":"","category":"section"},{"location":"apis/","page":"APIs","title":"APIs","text":"ClimaComms.start\nClimaComms.progress\nClimaComms.finish","category":"page"},{"location":"apis/#ClimaComms.start","page":"APIs","title":"ClimaComms.start","text":"start(ctx::AbstractGraphContext)\n\nInitiate graph data exchange.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.progress","page":"APIs","title":"ClimaComms.progress","text":"progress(ctx::AbstractGraphContext)\n\nDrive communication. Call after start to ensure that communication proceeds asynchronously.\n\n\n\n\n\n","category":"function"},{"location":"apis/#ClimaComms.finish","page":"APIs","title":"ClimaComms.finish","text":"finish(ctx::AbstractGraphContext)\n\nComplete the communications step begun by start(). After this returns, data received from all neighbors will be available in the stage areas of each neighbor's receive buffer.\n\n\n\n\n\n","category":"function"},{"location":"logging/#Logging","page":"Logging","title":"Logging","text":"","category":"section"},{"location":"logging/#Overview","page":"Logging","title":"Overview","text":"","category":"section"},{"location":"logging/","page":"Logging","title":"Logging","text":"Logging is a crucial tool for debugging, monitoring, and understanding the behavior of your applications. ClimaComms extends Julia's built-in logging functionality (provided by Logging.jl) to work seamlessly in distributed computing environments, particularly with MPI.","category":"page"},{"location":"logging/#Julia's-Logging-System","page":"Logging","title":"Julia's Logging System","text":"","category":"section"},{"location":"logging/","page":"Logging","title":"Logging","text":"Julia's standard library provides a flexible logging system through Logging.jl. Key concepts include:","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"Logger: An object that handles log messages, determining how they are formatted and where they are sent\nLog Level: Indicates the severity/importance of a message\nLog Record: Contains the message and associated metadata (level, module, line number, etc.)","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"The default logger in Julia (as of v1.11) is Logging.ConsoleLogger(), which prints messages to the console. You can check your current logger using:","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"using Logging; current_logger()","category":"page"},{"location":"logging/#ClimaComms-Logging-Features","page":"Logging","title":"ClimaComms Logging Features","text":"","category":"section"},{"location":"logging/","page":"Logging","title":"Logging","text":"ClimaComms builds upon Julia's logging system by providing specialized loggers for distributed computing.","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"To set any of the loggers below as the global logger, use Logging.global_logger(logger):","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"using ClimaComms, Logging\n\nctx = ClimaComms.context()\nlogger = ClimaComms.OnlyRootLogger(ctx)\nglobal_logger(logger)","category":"page"},{"location":"logging/#OnlyRootLogger","page":"Logging","title":"OnlyRootLogger","text":"","category":"section"},{"location":"logging/","page":"Logging","title":"Logging","text":"OnlyRootLogger(context) returns a logger that silences non-root processes. If using MPI, this logger is enabled on the first ClimaComms.init call.","category":"page"},{"location":"logging/#FileLogger","page":"Logging","title":"FileLogger","text":"","category":"section"},{"location":"logging/","page":"Logging","title":"Logging","text":"FileLogger(context, log_dir) writes to stdout and to a file log_dir/output.log simultaneously. If using MPI, the FileLogger separates logs by MPI process into different files, so that process i will write to the rank_$i.log file in the $log_dir directory, where $log_dir is of your choosing.  In this case, $log_dir/output.log is a symbolic link to the root process logs.  In other words, you can always look at $log_dir/output.log for the output. Logging to stdout can be disabled by setting the keyword argument log_stdout = false.","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"using ClimaComms, Logging\nctx = ClimaComms.context()\nlogger = ClimaComms.FileLogger(ctx, \"logs\")\n\nwith_logger(logger) do\n    @warn \"Memory usage high\"  # Written to rank-specific log file\nend","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"This will output the following in both the REPL and logs/rank_1.log:","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"┌ Warning: Memory usage high\n└ @ Main REPL[6]:2","category":"page"},{"location":"logging/#MPILogger","page":"Logging","title":"MPILogger","text":"","category":"section"},{"location":"logging/","page":"Logging","title":"Logging","text":"MPILogger(context) adds an MPI rank prefix to all log messages:","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"using ClimaComms, Logging\nctx = ClimaComms.context()\nlogger = MPILogger(ctx)\n\nwith_logger(logger) do\n    @info \"Processing data...\"  # Output: [P1]  Info: Processing data...\nend","category":"page"},{"location":"logging/#Log-Levels","page":"Logging","title":"Log Levels","text":"","category":"section"},{"location":"logging/","page":"Logging","title":"Logging","text":"Julia provides four standard log levels, in order of increasing severity:","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"Debug: Detailed information for debugging\nInfo: General information about program execution\nWarn: Warnings about potential issues\nError: Error conditions that might still allow the program to continue","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"See Julia documentation for more detailed information.","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"You can define custom log levels using LogLevel:","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"const Trace = LogLevel(-1000)  # Lower number = less severe\n@macroexpand @logmsg Trace \"Very detailed trace message\"","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"To disable all log messages at log levels equal to or less than a given LogLevel, use Logging.disable_logging(level).","category":"page"},{"location":"logging/#Filtering-Log-Messages","page":"Logging","title":"Filtering Log Messages","text":"","category":"section"},{"location":"logging/","page":"Logging","title":"Logging","text":"LoggingExtras.jl provides powerful filtering capabilities through the EarlyFilteredLogger(filter, logger), which takes two arguments:","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"filter(log_args) is a function which takes in log_args and returns a Boolean determining if the message should be logged. log_args is a NamedTuple with fields level, _module, id and group. Example filter functions are provided below in the Common Use Cases.\nlogger is any existing logger, such as Logging.ConsoleLogger() or MPILogger(ctx).","category":"page"},{"location":"logging/#Common-Use-Cases","page":"Logging","title":"Common Use Cases","text":"","category":"section"},{"location":"logging/#How-do-I-save-log-output-to-stdout-and-a-file-simultaneously?","page":"Logging","title":"How do I save log output to stdout and a file simultaneously?","text":"","category":"section"},{"location":"logging/","page":"Logging","title":"Logging","text":"ClimaComms.FileLogger logs to files and stdout simultaneously. For full customization, LoggingExtras.TeeLogger(loggers) composes multiple loggers, allowing for multiple loggers at once as shown below. ","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"using Logging, LoggingExtras\n\nio = open(\"simulation.log\", \"w\")\nloggers = (Logging.ConsoleLogger(stdout), Logging.ConsoleLogger(io))\ntee_logger = LoggingExtras.TeeLogger(loggers)\n\nwith_logger(tee_logger) do\n    @warn \"Log to stdout and file\"\nend\n\nclose(io)","category":"page"},{"location":"logging/#How-do-I-filter-out-warning-messages?","page":"Logging","title":"How do I filter out warning messages?","text":"","category":"section"},{"location":"logging/","page":"Logging","title":"Logging","text":"using Logging, LoggingExtras\n\nfunction no_warnings(log_args)\n    return log_args.level != Logging.Warn\nend\n\nfiltered_logger = EarlyFilteredLogger(no_warnings, Logging.current_logger())\n\nwith_logger(filtered_logger) do\n    @warn \"Hide this warning\"\n    @info \"Display this message\"\nend","category":"page"},{"location":"logging/#How-do-I-filter-out-messages-from-certain-modules?","page":"Logging","title":"How do I filter out messages from certain modules?","text":"","category":"section"},{"location":"logging/","page":"Logging","title":"Logging","text":"We can create a custom filter that returns false if a log message originates from a list of excluded modules.","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"The same pattern can be reversed to filter messages only coming from certain modules.","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"using Logging, LoggingExtras\n\nmodule_filter(excluded_modules) = log_args ->\n    !(log_args._module in excluded_modules)\n\nModuleFilteredLogger(excluded) =\n    EarlyFilteredLogger(module_filter(excluded), Logging.current_logger())\n# To test this logger:\nmodule TestModule\n    using Logging\n    function log_something()\n        @info \"This message will appear\"\n    end\nend\n\nexcluded = (Main, Base)\nwith_logger(ModuleFilteredLogger(excluded)) do\n    @info \"Hide this message\"\n    TestModule.log_something()\nend","category":"page"},{"location":"logging/","page":"Logging","title":"Logging","text":"[ Info: This message will appear","category":"page"},{"location":"faqs/#Frequently-Asked-Questions","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"faqs/#How-do-I-run-my-simulation-on-a-GPU?","page":"Frequently Asked Questions","title":"How do I run my simulation on a GPU?","text":"","category":"section"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Set the environment variable CLIMACOMMS_DEVICE to CUDA. This can be accomplished in your Julia script with (at the top)","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"ENV[\"CLIMACOMMS_DEVICE\"] = \"CUDA\"","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"or calling","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"export CLIMACOMMS_DEVICE=\"CUDA\"","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"in your shell (outside of Julia, no spaces).","category":"page"},{"location":"faqs/#My-simulation-does-not-start-and-crashes-with-a-MPI-error.-I-don't-want-to-run-with-MPI.-What-should-I-do?","page":"Frequently Asked Questions","title":"My simulation does not start and crashes with a MPI error. I don't want to run with MPI. What should I do?","text":"","category":"section"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"ClimaComms tries to be smart and select the best configuration for your run. Sometimes, it fails with an error message like the following.","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"cmd=init pmi_version=2 pmi_subversion=0\n--------------------------------------------------------------------------\nPMI2_Init failed to intialize.  Return code: 14\n--------------------------------------------------------------------------\n--------------------------------------------------------------------------\nThe application appears to have been direct launched using \"srun\",\nbut OMPI was not built with SLURM's PMI support and therefore cannot\nexecute. There are several options for building PMI support under\nSLURM, depending upon the SLURM version you are using:\n\n  version 16.05 or later: you can use SLURM's PMIx support. This\n  requires that you configure and build SLURM --with-pmix.\n\n  Versions earlier than 16.05: you must use either SLURM's PMI-1 or\n  PMI-2 support. SLURM builds PMI-1 by default, or you can manually\n  install PMI-2. You must then build Open MPI using --with-pmi pointing\n  to the SLURM PMI library location.\n\nPlease configure as appropriate and try again.\n--------------------------------------------------------------------------\n*** An error occurred in MPI_Init_thread\n*** on a NULL communicator\n*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,\n***    and potentially your MPI job)","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"In this case, you can force ClimaComms to ignore MPI with","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"ENV[\"CLIMACOMMS_CONTEXT\"] = \"SINGLETON\"","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"at the top of your Julia script or by calling","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"export CLIMACOMMS_CONTEXT=\"SINGLETON\"","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"in your shell (outside of Julia, no spaces).","category":"page"},{"location":"faqs/#My-code-is-saying-something-about-ClimaComms.@import_required_backends,-what-does-that-mean?","page":"Frequently Asked Questions","title":"My code is saying something about ClimaComms.@import_required_backends, what does that mean?","text":"","category":"section"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"When you are using the environment variables to control the execution of your script, ClimaComms can detect that some important packages are not loaded. For example, ClimaComms will emit an error if you set CLIMACOMMS_DEVICE=\"CUDA\" but do not import CUDA.jl in your code.","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"ClimaComms provides a macro, ClimaComms.@import_required_backends, that you can add at the top of your scripts to automatically load the required packages when needed. Note, the packages have to be in your Julia environment, so you might install packages like MPI.jl and CUDA.jl.","category":"page"},{"location":"faqs/#How-can-I-see-the-MPI-state-and-verify-that-MPI-is-set-up-correctly?","page":"Frequently Asked Questions","title":"How can I see the MPI state and verify that MPI is set up correctly?","text":"","category":"section"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"To inspect the current MPI state, use the summary function: print(summary(context))","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The output varies depending on your communication context type:","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"SingletonCommsContext: Displays basic context information and device type\nMPICommsContext: Shows detailed information including each node's rank.","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"When using GPU acceleration with CUDADevice, the summary additionally includes the device type and UUID.","category":"page"},{"location":"faqs/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"To test that MPI and CUDA are set up correctly, see this guide.","category":"page"},{"location":"#ClimaComms","page":"Home","title":"ClimaComms","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"ClimaComms.jl is a small package that provides abstractions for different computing devices and environments. ClimaComms.jl is use extensively by CliMA packages to control where and how simulations are run (e.g., one on core, on multiple GPUs, et cetera).","category":"page"},{"location":"","page":"Home","title":"Home","text":"This page highlights the most important user-facing ClimaComms concepts. If you are using ClimaComms as a developer, refer to the Developing with ClimaComms page. For a detailed list of all the functions and objects implemented, the APIs page collects all of them.","category":"page"},{"location":"#Devices-and-Contexts","page":"Home","title":"Devices and Contexts","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The two most important objects in ClimaComms.jl are the Device and the Context.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A Device identifies a computing device, a piece of hardware that will be executing some code. The Devices currently implemented are","category":"page"},{"location":"","page":"Home","title":"Home","text":"CPUSingleThreaded for a CPU core with a single thread,\nCPUMultiThreaded for a CPU core with multiple threads,\nCUDADevice for a single CUDA-enabled GPU.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Devices are part of Contexts, objects that contain information require for multiple Devices to communicate. Implemented Contexts are","category":"page"},{"location":"","page":"Home","title":"Home","text":"SingletonCommsContext, when there is no parallelism;\nMPICommsContext , for a MPI-parallelized runs.","category":"page"},{"location":"","page":"Home","title":"Home","text":"To choose a device and a context, most CliMA packages use the device() and context() functions. These functions look at specific environment variables and set the device and context accordingly. By default, the CPUSingleThreaded device is chosen and the context is set to SingletonCommsContext unless ClimaComms detects being run in a standard MPI launcher (as srun or mpiexec).","category":"page"},{"location":"","page":"Home","title":"Home","text":"For example, to run a simulation on a GPU, run julia as","category":"page"},{"location":"","page":"Home","title":"Home","text":"export CLIMACOMMS_DEVICE=\"CUDA\"\nexport CLIMACOMMS_CONTEXT=\"SINGLETON\"\n# call/open julia as usual","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nThere might be other ways to control the device and context. Please, refer to the documentation of the specific package to learn more.","category":"page"},{"location":"#Running-with-MPI/CUDA","page":"Home","title":"Running with MPI/CUDA","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"CliMA packages do not depend directly on MPI or CUDA, so, if you want to run your simulation in parallel mode and/or on GPUs, you will need to install some packages separately.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For parallel simulations, MPI.jl, and for GPU runs, CUDA.jl. You can install these packages in your base environment","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia -E \"using Pkg; Pkg.add(\\\"CUDA\\\"); Pkg.add(\\\"MPI\\\")\"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Some packages come with environments that includes all possible backends (typically .buildkite). You can also consider directly using those environments.","category":"page"},{"location":"#Writing-generic-kernels","page":"Home","title":"Writing generic kernels","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"To implement kernels that work across all Devices, ClimaComms.jl provides the @threaded macro. Like the standard library's Threads.@threads macro, this can be placed in front of any for-loop to parallelize its iterations across threads. For example, given two vectors a and b of equal size, a threaded version of the copyto! function can be implemented as","category":"page"},{"location":"","page":"Home","title":"Home","text":"julia> threaded_copyto!(a, b) = ClimaComms.@threaded for i in axes(a, 1)\n           a[i] = b[i]\n       end\nthreaded_copyto! (generic function with 1 method)\n\njulia> threaded_copyto!(a, b)","category":"page"},{"location":"","page":"Home","title":"Home","text":"This macro offers several options for fine-tuning performance:","category":"page"},{"location":"","page":"Home","title":"Home","text":"the Device can be specified to reduce compilation time\nthe level of thread coarsening (number of loop iterations evaluated in each thread) can be specified to reduce the overhead of launching many threads\non GPUs, the size of each block (a collection of threads launched on a single multiprocessor) can be specified to improve GPU utilization","category":"page"},{"location":"","page":"Home","title":"Home","text":"note: Note\nExecuting code on GPUs requires static compilation, which means that the compiler must be able to infer the types of all variables used in a threaded loop. For example, global variables and type variables defined outside a loop should be avoided when the loop is parallelized on a GPU. See the docstring of @threaded for more information.","category":"page"},{"location":"internals/#Developing-with-ClimaComms","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"","category":"section"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"This page goes into more depth about ClimaComms in CliMA packages.","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"First, we will describe what Devices and Contexts are.","category":"page"},{"location":"internals/#Devices","page":"Developing with ClimaComms","title":"Devices","text":"","category":"section"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"Devices identify a specific type of computing hardware (e.g., a CPU/a NVidia GPU, et cetera). The Devices implemented are","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"CPUSingleThreaded, for a CPU core with a single thread;\nCUDADevice, for a single CUDA GPU.","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"Devices in ClimaComms are singletons, meaning that they do not contain any data: they are used exclusively to determine what implementation of a given function or data structure should be used. For example, let us implement a function to allocate an array what on the CPU or on the GPU depending on the Device:","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"import ClimaComms: CPUSingleThreaded, CUDADevice\nimport CUDA\n\nfunction allocate(device::CPUSingleThreaded, what)\n    return Array(what)\nend\n\nfunction allocate(device::CUDADevice, what)\n    return CUDA.CuArray(what)\nend","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"If we want to allocate memory on the GPU, we would do something like:","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"julia> allocate(CUDADevice(), [1, 2, 3])\nCUDA.CuArray([1, 2, 3])","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"Low-level CliMA code often needs to implement different methods for different Devices (e.g., in ClimaCore), but this level of specialization is often not required at higher levels.","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"Higher-level code often interacts with Devices through ClimaComms functions such time or sync. These functions implement device-agnostic operations. For instance, the proper way to compute how long a given expression takes to compute is","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"import ClimaComms: @time\n\ndevice = ClimaComms.device()  # or the device of interest\n\n@time device my_expr","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"This will ensure that the correct time is computed (e.g., the time to run a GPU kernel, and not the time to launch the kernel).","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"For a complete list of such functions, consult the APIs page.","category":"page"},{"location":"internals/#Contexts","page":"Developing with ClimaComms","title":"Contexts","text":"","category":"section"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"A Context contains the information needed for multiple devices to communicate. For simulations with only one device, SingletonCommsContext simply contains an instance of an AbstractDevice. For MPI simulations, the context contains the MPI communicator as well.","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"Contexts specify devices and form of parallelism, so they are often passed around in both low-level and higher-level code.","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"ClimaComms provide functions that are context-agnostic. For instance, reduce applies a given function to an array across difference processes and collects the result. Let us see an example","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"import ClimaComms\nClimaComms.@import_required_backends\n\ncontext = ClimaComms.context()  # Default context (read from environment variables)\ndevice = ClimaComms.device(context)  # Default device\n\nmypid, nprocs = ClimaComms.init(context)\n\nArrayType = ClimaComms.array_type(device)\n\nmy_array = mypid * ArrayType([1, 1, 1])\n\nreduced_array = ClimaComms.reduce(context, my_array, +)\nClimaComms.iamroot(context) && @show reduced_array","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"@import_required_backends is responsible for loading relevant libraries, for more information refer to the Backends and extensions section.","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"In this snippet, we obtained the default context from environment variables using the context function. As developers, we do not know whether this code is being run on a single process or multiple, so took the more generic stance that the code might be run on several processes. When several processes are being used, the same code is being run by parallel Julia instances, each with a different process id (pid). The line julia mypid, nprocs = ClimaComms.init(context) assigns different mypid to the various processes and returns nprocs so that we can use this information if needed. This function is also responsible for distributing GPUs across processes, if relevant.","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"In this example, we used mypid to set up my_array in such a way that it would be different on different processes. We set up the array with ArrayType, obtained with array_type. This function provides the type to allocate the array on the correct device (CPU or GPU). Then, we applied reduce to sum them all. reduce collects the result to the root process, the one with pid = 1. For single-process runs, the only process is also a root process.","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"The code above works independently on the number of processes and over all the devices supported by ClimaComms.","category":"page"},{"location":"internals/#Backends-and-extensions","page":"Developing with ClimaComms","title":"Backends and extensions","text":"","category":"section"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"Except the most basic ones, ClimaComms computing devices and contexts are implemented as independent backends. For instance, ClimaComms provides an AbstractDevice interface for which CUDADevice is an implementation that depends on CUDA.jl. Scripts that use ClimaComms have to load the packages that power the desired backend (e.g., CUDA.jl has to be explicitly loaded if one wants to use CUDADevices).","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"When using the default context and device (as read from environment variables), ClimaComms can automatically load the required packages. To instruct ClimaComms to do, add ClimaComms.@import_required_backends at the beginning of your scripts. ClimaComms can also identify some cases when a package is required but not loaded and warn you about it.","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"Using non-trivial backends might require you to install CUDA.jl and/or MPI.jl in your environment.","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"Note: When using context to select the context, it is safe to always add ClimaComms.@import_required_backends at the top of your scripts. Do not add ClimaComms.@import_required_backends to library code (i.e., in src) because the macro requires dependencies that should not be satisfied in that instance.","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"Technically, ClimaComms backends are implemented in Julia extensions. There are two main reasons for this:","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"optional packages (such as CUDA and MPI) should not be hard dependencies of ClimaComms;\nimplementing code in extensions reduces the loading time for ClimaComms and downstream packages because it skips compilation of code that is not needed.","category":"page"},{"location":"internals/","page":"Developing with ClimaComms","title":"Developing with ClimaComms","text":"If you are implementing features for ClimaComms, make sure that your backend-specific code is in a Julia extension (in the ext folder).","category":"page"}]
}
